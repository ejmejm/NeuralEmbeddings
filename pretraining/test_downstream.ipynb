{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from config_handling import prepare_config\n",
    "from data_loading import prepare_downsteam_dataloaders\n",
    "from models import NeuroSignalEncoder, NeuroDecoder, PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDecoderCustom(nn.Module):\n",
    "    def __init__(self,\n",
    "                 config: dict,\n",
    "                 encoder: Optional[NeuroSignalEncoder] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model_config = config['model']\n",
    "        self.downstream_config = config['downstream']\n",
    "\n",
    "        # Load the encoder model if necessary\n",
    "        if encoder is None:\n",
    "            self.encoder = NeuroSignalEncoder(self.model_config)\n",
    "            \n",
    "            # Add LSTM layer to model before loading weights if CPC was used\n",
    "            if self.config['train_method'].lower() == 'cpc':\n",
    "                self.encoder.add_lstm_head(self.model_config['lstm_embedding_dim'])\n",
    "                self.encoder.load_state_dict(torch.load(self.model_config['save_path']))\n",
    "            else:\n",
    "                # MSM was not trained with the LSTM head, so it should be added after\n",
    "                # the weights for the rest of the model are loaded\n",
    "                self.encoder.load_state_dict(torch.load(self.model_config['save_path']))\n",
    "                self.encoder.add_lstm_head(self.model_config['lstm_embedding_dim'])\n",
    "        else:\n",
    "            self.encoder = encoder\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.model_config['lstm_embedding_dim'], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.downstream_config['n_classes']))\n",
    "\n",
    "    def forward(self,\n",
    "                primary_input: Tensor,\n",
    "                calibration_input: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder, that takes stimulus inputs and outputs\n",
    "        a predicted class.\n",
    "\n",
    "        Args:\n",
    "            primary_input: Tensor of shape (batch_size, n_timesteps, n_channels)\n",
    "            calibration_input: Tensor of shape (1, n_timesteps, n_channels)\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        # Pass the input signals through the encoder\n",
    "        output_dict = self.encoder(primary_input, calibration_input=calibration_input)\n",
    "\n",
    "        lstm_embeds = output_dict['lstm_embeddings'] # Sequence of all hidden outputs\n",
    "        primary_mask = output_dict['primary_embeddings_mask']\n",
    "\n",
    "        # Select the embeddings corresponding to the primary sequence\n",
    "        selected_embeds = lstm_embeds.masked_select(primary_mask.unsqueeze(-1).bool())\n",
    "        primary_embeds = selected_embeds.reshape(lstm_embeds.shape[0], -1, lstm_embeds.shape[2])\n",
    "\n",
    "        # Select the emebddings at the end of the stimulus response time\n",
    "        target_output_idx = torch.tensor(\n",
    "            (self.downstream_config['n_stimulus_samples'] / \\\n",
    "            self.downstream_config['tmax_samples']) \\\n",
    "            * primary_embeds.shape[1]).ceil().type(torch.int64)\n",
    "        target_output_idx = target_output_idx.to(primary_embeds.device)\n",
    "        output_embeds = primary_embeds.index_select(dim=1, index=target_output_idx)\n",
    "        output_embeds = output_embeds.squeeze(1)\n",
    "\n",
    "        # Pass the embeddings through the decoder\n",
    "        class_logits = self.decoder(output_embeds)\n",
    "\n",
    "        return class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(700*204, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat((y[:, -200:], x[:, :500]), axis=1)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.fc_layers(x)\n",
    "      \n",
    "class TestModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(204, 256, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(20*256, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat((y[:, -200:], x[:, :500]), axis=1)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.fc_layers(x)\n",
    "     \n",
    "     \n",
    "class TestModel3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq_len = 24\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(204, 128, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.tlayer = nn.TransformerEncoderLayer(\n",
    "                d_model = 128,\n",
    "                nhead = 2,\n",
    "                dim_feedforward = 64,\n",
    "                # dropout = dropout,\n",
    "                activation = 'relu',\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(self.tlayer, 1)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.seq_len*128, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10))\n",
    "            \n",
    "        self.pos_enc = PositionalEncoding(\n",
    "            128, 0.25 * 0.1, self.seq_len)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat((y[:, -200:], x[:, :600]), axis=1)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        # x = self.pos_enc(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = prepare_config('configs/w2v_cpc.yaml')\n",
    "model_config = config['model']\n",
    "# encoder = NeuroSignalEncoder(model_config)\n",
    "# encoder.add_lstm_head(model_config['lstm_embedding_dim'])\n",
    "# encoder.load_state_dict(torch.load(model_config['save_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_inputs(inputs: List[Tensor], config: Dict) -> List[Tensor]:\n",
    "    \"\"\"\n",
    "    Reshapes the inputs and moves them to the correct device.\n",
    "\n",
    "    Args:\n",
    "        inputs: List of tensors of shape (batch_size, n_channels, n_timesteps)\n",
    "            or (n_channels, n_timesteps)\n",
    "        config: Dictionary containing the config\n",
    "\n",
    "    Returns:\n",
    "        List of tensors of shape (batch_size, n_timesteps, n_channels)\n",
    "    \"\"\"\n",
    "    new_inputs = []\n",
    "    for i in range(len(inputs)):\n",
    "        new_inputs.append(inputs[i])\n",
    "\n",
    "        # If the inputs are in the shape (n_channels, n_timesteps)\n",
    "        if len(new_inputs[i].shape) == 2:\n",
    "            new_inputs[i] = new_inputs[i].unsqueeze(0)\n",
    "\n",
    "        # Reshape the inputs to (batch_size, n_timesteps, n_channels)\n",
    "        new_inputs[i] = new_inputs[i].permute(0, 2, 1)\n",
    "\n",
    "        # Move the inputs to the correct device\n",
    "        new_inputs[i] = new_inputs[i].to(config['device'])\n",
    "\n",
    "    return new_inputs\n",
    "\n",
    "# TODO: This is hardcoded for the MEG colors dataset, fix that\n",
    "def format_labels(labels: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Reshapes the labels, moves them to the correct device,\n",
    "    and converts the range to the range [0, n_classes - 1].\n",
    "\n",
    "    Args:\n",
    "        labels: Tensor of shape (batch_size, 1)\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    labels = labels.squeeze(1)\n",
    "    labels = labels - 1\n",
    "    labels = labels.to(config['device'])\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(NeuroDecoder(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TestModel3())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in encoder.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    }
   ],
   "source": [
    "ds_config = config['downstream']\n",
    "\n",
    "#Prepare the data\n",
    "batch_size = 16\n",
    "dataloaders = prepare_downsteam_dataloaders(config, batch_size)\n",
    "train_loader = dataloaders['train']\n",
    "val_loader = dataloaders['val']\n",
    "test_loader = dataloaders['test']\n",
    "\n",
    "model = TestModel3() # NeuroDecoder(config) # \n",
    "model = model.to(config['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002) # ds_config['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(2): # ds_config['train_epochs']):\n",
    "    print('Starting epoch', i+1)\n",
    "    epoch_preds = []\n",
    "    epoch_labels = []\n",
    "    epoch_losses = []\n",
    "\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    for sample_idx, data in enumerate(train_loader):\n",
    "        # Get and format the data for the batch\n",
    "        primary_input = data['primary_input']\n",
    "        calibration_input = data['calibration_input']\n",
    "        labels = data['label']\n",
    "\n",
    "        primary_input, calibration_input = format_inputs(\n",
    "            (primary_input, calibration_input), config)\n",
    "        labels = format_labels(labels)\n",
    "\n",
    "        # Run the data through the model\n",
    "        logits = model(primary_input, calibration_input)\n",
    "\n",
    "        # Update the result buffers\n",
    "        preds = logits.argmax(dim=1)\n",
    "        epoch_preds.extend(preds.detach().cpu().numpy())\n",
    "        epoch_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        # Calculate the loss and update the model weights\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        loss = criterion(probs, labels)\n",
    "        batch_losses.append(loss)\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        if sample_idx % 16 == 0:\n",
    "            batch_loss = torch.mean(torch.stack(batch_losses))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses = []\n",
    "\n",
    "        # Print updates\n",
    "        if (sample_idx + 1) % 32 == 0:\n",
    "            lookback = batch_size * 32\n",
    "            print(sample_idx, np.mean(epoch_losses[-32:]))\n",
    "            print('Accuracy:',\n",
    "                (np.array(epoch_preds)[-lookback:] == \\\n",
    "                 np.array(epoch_labels)[-lookback:]).sum() \\\n",
    "                    / len(epoch_preds[-lookback:]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6717cf457fe527f2ad07ab71b4770f157b357bf37d07e7427487ba89b10c0212"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
