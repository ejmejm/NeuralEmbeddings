#############################################
###                                       ###
###  NeuralEmbeddings Pretraining Config  ###
###                                       ###
#############################################

### TODO ###

seed: null

### Weights & Biases Integration ###

wandb_mode: "disabled" # ["online", "offline", "disabled"]
wandb_entity: "652_neural_embed_team"
wandb_project: "neural-embeddings"

### Training ###

train_epochs: 10
learning_rate: 0.001
log_interval: 64
val_interval: 512
device: "cuda"

# The method params only need to be defined
# for the method being used
msm_params:
  mask_prob: 0.1
  min_mask_len: 2
  max_mask_len: 4

  # These learning rates are overwriten by the global
  # lr if not defined or null
  sc_lr: 0.001
  mc_lr: 0.001
  calib_lr: 0.001
  scheduler_enabled: True
  phase_decay: 0.1
  
cpc_params:
  embedding_dim: 128
  n_pred_steps: 12
  mi_seq_radius: null # Radius of sequence embeddings to use for loss calculation
  scheduler_enabled: True

### Data ###

data_type: "grad" # Valid values: [MEG, EEG, grad, mag]

# Number of samples = unit size * batch size
# The number of sequential points that are sampled for a single example
primary_unit_size: 1000
calibration_unit_size: 5000

# Total sequential points per batch =
# batch_size * primary_unit_size + calibration_unit_size
batch_size: 8
# Whether or not to shuffle the samples within a batch
shuffle_in_batch: True

val_split: 0.02 # Percentage of data used for validation
# test_split is currently not being used
test_split: 0.15 # Percentage of data used for testing

### Preprocessing ###

data_file_type: ".fif"

train_dir: "data/train_datasets"
test_dir: "data/test_datasets"

common_sfreq: 1000.0
high_pass_cut_off: 0.1
high_pass_filter_length: "auto"
# In milliseconds because sample frequency is 1000 Hz
min_recording_length: 33000

# Only one of standardization and normalization can be used
use_standardization: True
use_normalization: False
use_high_pass_filter: True

# ICA
use_ica: False
ica:
  n_components: 0.95 # Min fraction variance explained
  max_iter: 500

# Number of samples used to fit preprocessors
preprocess_fit_samples: 5
# Save path for preprocessed models
preprocessed_model_path: "../models/main_preprocessors.pkl"

# Save paths for preprocessed data
train_val_preprocessed: "data/train_preprocessed"
test_preprocessed: "data/test_preprocessed"

# Save filename for metadata (wihout filetype)
train_val_info: "dataset_info"
test_info: "dataset_info"


# Epoched data sample size
tmin_samples: 200
tmax_samples: 600


### Model ###


model:
  save_path: "../models/model.pt"

  max_primary_input_len: 1000
  max_calibration_input_len: 5000
  embedding_dim: 32
  channel_combine_func: "mean" # Valid values: [mean, logsumexp]

  ### Primary Model ###

  # Single channel module params
  single_channel_module:
    enabled: True

    # Conv
    filter_size: 100
    stride: 75

    # Transformer
    n_layers: 2
    dropout: 0.4
    n_head: 2 # N heads for the attention
    feedforward_dim: 128

  # Mixed channel module parameters
  mixed_channel_module:
    enabled: True

    # Conv (NOT USED unless the single channel module is disabled)
    filter_size: 100
    stride: 75

    # Transformer
    n_layers: 2
    dropout: 0.4
    n_head: 2 # N heads for the attention
    feedforward_dim: 128

  ### Calibration Model ###
  
  # Calibration module params
  calibration_module:
    enabled: True

    # Conv
    filter_size: 100
    stride: 75

    # Transformer
    n_layers: 2
    dropout: 0.4
    n_head: 2 # N heads for the attention
    feedforward_dim: 128