#############################################
###                                       ###
###  NeuralEmbeddings Pretraining Config  ###
###                                       ###
#############################################


seed: 0


### Training ###


train_epochs: 50
learning_rate: 0.001
log_interval: 10
device: "cuda"

train_method: "MSM"

# The method params only need to be defined
# for the method being used
msm_params:
  mask_prob: 0.15
  min_mask_len: 2
  max_mask_len: 8
  
cpc_params:
  wip: True

swav_params:
  wip: True


### Data ###

data_type: "MEG" # Valid values: [MEG, EEG]

# Number of samples = unit size * batch size
# seq_unit_size: 2048
# The number of sequential points that are sampled for a single example
primary_unit_size: 512
calibration_unit_size: 1024

# Total sequential points per batch =
# batch_size * primary_unit_size + calibration_unit_size
batch_size: 2


val_split: 0.05 # Percentage of data used for validation
test_split: 0.15 # Percentage of data used for testing


### Model ###


model:
  save_path: "../models/test_model.pt"

  max_primary_input_len: 512
  max_calibration_input_len: 1024
  embedding_dim: 16
  channel_combine_func: "mean" # Valid values: [mean, logsumexp]

  ### Primary Model ###

  # Single channel module params
  single_channel_module:
    enabled: True

    # Conv
    filter_size: 32
    stride: 16

    # Transformer
    n_layers: 2
    dropout: 0.4
    n_head: 2 # N heads for the attention
    feedforward_dim: 256

  # Mixed channel module parameters
  mixed_channel_module:
    enabled: True

    # Conv (NOT USED unless the single channel module is disabled)
    filter_size: 32
    stride: 16

    # Transformer
    n_layers: 2
    dropout: 0.4
    n_head: 2 # N heads for the attention
    feedforward_dim: 256

  ### Calibration Model ###
  
  # Calibration module params
  calibration_module:
    enabled: True

    # Conv
    filter_size: 64
    stride: 32

    # Transformer
    n_layers: 2
    dropout: 0.4
    n_head: 2 # N heads for the attention
    feedforward_dim: 256